\section{A lens modeling challenge} \label{sec:mod_challenge}

% \subsection{\sw} \label{sec:SpaceWarps} some words about \sw

Interested volunteers from the \sw forum were initially introduced to
\spl through a video tutorial and by videocon.  After this
introductory stage, a modeling challenge was presented.  This
consisted of 29 simulated lenses (sims) covering a range of lensing
configurations.

To estimate the performance of the volunteers and the quality of the generated models, two tests T1 and T2 were done.

The second test T2 was to compare the desired results of a modeling process, the mass distribution of the lens $\kappa(x, y)$.

The \sw sims were generated by AM, in consultation with PM and AV.


\subsection{The simulated lenses} \label{sec:sims}

In the interest of blind testing, the information in this section was
revealed neither to RK, while choosing the challenge set of 29 sims,
nor to the modellers (volunteers EB, CC, CM, JO, JW and `expert
modeller' PS) until the modelling stage was done.

The sims were produced using {\tt gravlens}
\citep{2001astro.ph..2341K,2001astro.ph..2340K}.  The were of three
kinds, as follows.

\begin{enumerate}
  \item Imitating lensed quasars: having a singular elliptical
    isothermal lens (SIE) plus constant external shear, and a circular
    Gaussian source.
  \item Emulating lensed galaxies: similar to the above, but with an
    elliptical de Vaucouleurs source.
  \item Resembling cluster lenses: having a source as above, but a
    more complicated lens, with one dominant elliptical SIE and and
    one or more perturbing elliptical SIEs, plus a circular NFW
    \citep{1996ApJ...462..563N,1997ApJ...490..493N} to represent
    the intergalactic mass.
\end{enumerate}

Formulas for the lenses appear in \cite{2001astro.ph..2341K}. The SIE
lenses follow equations (33--35) of that work, with core radius set to
zero.  The NFW lens is in equations (48) and (50), while shear is the
$\gamma$ term in equation (76).

\subsection{Some example models} \label{sec:example_models}

The modellers proffered a total of 129 models for the 29 sims in the
challenge.  Figures \ref{fig:6941} to \ref{fig:6915} show eight of
models in some detail.

Figure \ref{fig:6941} shows a simple example.  The top two panels show
contours of convergence $\kappa(x,y)$, and of the arrival-time for a
point source at the brightest point of the sim
source.\footnote{Although the arrival-time contours represent an
  abstract quantity that is not directly observable, the contour map
  may actually resemble the appearance of the lensed arcs.  The
  resemblance is due to a serendipitous computer-graphical effect
  \citep{2001AJ....122..585S}.} The spatial scale is in pixels.  The
information in these two panels was, of course, kept secret during the
modelling challenge.  The panels in the middle row and at the lower
right are the results that \spl returns to the modeller, in order for
them to assess the model.  These derive from the mean of an ensemble
of 200 models generated by \spl.  The model mass distribution is
returned as a contour map superimposed on an intensity map. A fairly
smooth mass distribution, as here, is a good sign.  An irregular or
checkboard pattern in the mass map usually indicates a bad model.  The
model arrival-time contours.  The special contour passing through a
saddle point is shown in black: this is the model version of the
spaghetti sketch provided by the user.  Also provided is a synthetic
image of the lensed source.  For this, \spl assumes a simple conical
source profile.  The user can change the contrast level on the image,
which (though it is not saved) amounts to to adjusting the width of
the cone.  Finally, to the lower left of the figure, we have a
comparison of the input and recovered mass profiles.  The panel shows
a average $\kappa$ within a given radius, as a function of radius.
The red curve is the true value, and where it crosses unity (dotted
horizonal line) is the notional Einstein radius.  The two blue curves
are the minimal and maximal mean enclosed $\kappa$ from the internal
ensemble in \spl.  The region between the blue curves is shaded
between the radii of the innermost and the outermost images --- this
is the confidence region from the modelling.  As we see, the shaded
blue is slightly above the red curve.  In summary: the identification
of minimum and saddle point is correct, but the estimated Einstein
radius is a little too high.

Figure \ref{fig:6975} shows a lens with substructure in the form of a
smaller secondary galaxy.  The galaxies in such group or cluster sims
were, in fact, based on galaxies visible in the images --- but the
modellers were not told in advance whether this was the case.  The
model does not include any substructure, but otherwise is not bad.
The minimum and saddle point are correct, and the Einstein radius is
only a little underestimated.

Figure \ref{fig:6937} shows a case where substructure leads to a
poor model.

Figure \ref{fig:6975} shows a fairly symmetric quad.  The minima and
saddle points are correctly identified, and the orientation of the
ellipticity of the mass distribution is correctly reproduced.  The
Einstein radius is somewhat overestimated.  Figure \ref{fig:7022}
shows another model of the same system.  In this one, the
identification of the minima and saddle points was incorrect, and mass
distribution comes out elongated East-West instead of North-South.
The mass distribution also appears somewhat jagged and the
saddle-point contours are not as clean as in the previous examples;
these are often indicators of a problem with the model.  The enclosed
mass is, however, none the worse --- the reason is probably that in a
relatively symmetric image configuration, the Einstein radius is quite
well constrained by the images in a fairly model-independent way.

Figure \ref{fig:6990} shows a long-axis quad.

Figure \ref{fig:6919} shows a short-axis quad.

Figure \ref{fig:6915} shows an inclined quad.

\subsection{Test of image identification} \label{sec:tests.t1}

T1 tested the volunteers ability to reconstruct the arrival time surface given a survey image containing a sim.
This task consists of two parts.
First, the correct identification and location of lensed images (T1a).
Second the correct ordering for the identified lensed images in respect of the arrival time (T1b).

While we expected T1a to be trivial, given the nature of the survey images and the success of \sw, we expect T1b to be more difficult.
T1b tests the volunteers understandings of the theory of arrival time surfaces and the odd number theorem.
While we can provide the volunteers with some general rules of thumb, T1b involves imagination and guessing and therefore training could improve their skills in a later stage.

T1 was also designed to give some feedback on the difficulties volunteers encounter, to further improve the tutorial materials.

The second test T2 was to compare the desired results of a modeling process, the mass distribution of the lens $\kappa(x, y)$.
To get a means of comparing the simulated data (sims) to the generated modeled data (models), the total convergence, also called enclosed mass $\kappa_{\text{encl}}(r)$ for both was calculated.
The Einstein radius $\Theta_E$ is defined by $\kappa_{\text{encl}}(\Theta_E)=1$ and gives a number that allows the comparison between the sims and the models.
We also let an expert model three selected systems to compare the results from volunteers to those of a professional.

\todo{!} Should I write down prior expectation for T2 too?


\subsection{Test of image identification} \label{sec:tests.t1}

The evaluation of the volunteers performance for T1 was done manually, comparing their input from \spl and the resulting reconstructed arrival time surface contour line plot (arrival plot) to the arrival plot generated using simulation parameters.
% \Figref{output_compare} shows the setup used to evaluate the models.


To evaluate T1a and T1b, each model was evaluated with two binary tests.
The first is passed, if all the images have been identified and are approx. within $\pm0.05\cdot\text{imgage width}$.
The second test is passed, if those identified points have the right parity, ordering with respect to arrival time.

Additionally, ten types of errors (E01 -- E10, listed in \tabref{stats}) that could occur were analyzed, where each model could contain multiple of those.

% The complete table with the results for each model can be found in
% the appendix, \tabref{detail_results}.

%\begin{enumerate}
%1  \item inaccurate placement in an extended arc
%2  \item wrongly identified sad and min in 3 image configuration
%3  \item identified only 3 instead of 5 images
%4  \item tried to model an arc with a min instead of min-sad-min
%5  \item PI-err (rotation by 180 degrees; in 5 image configuration, exchanged the ordering of the two saddle points)
%6  \item PI/2-err (rotation by 90 degree; sad$\longmapsto$min$\longmapsto$sad$\longmapsto$min$\longmapsto$sad)
%7  \item missed faint image(s)
%8  \item tried to model an arc with min-sad-min instead of only min
%9  \item did identify two close by images as one
%10  \item used 7 or more image to model a 5 image system.
%\end{enumerate}


In \tabref{stats} a summary of this evaluation is presented.
We conclude that the volunteers are performing very well identifying and positioning images (T1a), with a performance of 92\% (R1, p=0.92).
Most of the problems where due to unclear arc-like structures (E01, p=0.18; E04, p=0.03; E08, p=0.04).
Critical errors like the failure to identify all five images in a five images system (E03, p=0.04) or to include too many images (E10, p=0.01) did almost never happen.
From this we conclude that the introduction materials was adequate and the volunteers understand the basics of gravitational lensing.

\todo{add more detail?} Add more details like total number of images detected / tot am images (rightPlace fraction)??

The assignment of the parity of the images (T1b) was a more difficult task.
In 59\% (R2, p=0.59, N=70) of the cases the volunteers succeeded to identify the right configuration.
Most of the failures are due to E06, with N=38, p=32\%, followed by E05 (PI) with N=7, p=6\%.
For an example of E06, see \figref{7022}.
It basically describes a situation, where the minima and saddle points of a five image configuration were exchanged (rotated by $\pm90\dgr$).
E05 describes the situation, where the ordering of the saddle points was wrong (rotation by $180\dgr$).
While these errors occurred, we suspect they can be avoided with better training material and some examples for the obvious cases.
For more challenging cases, like very symmetrical distribution of the lensed images (for example model 7022, \figref{7022}), those errors should still produce plausible results, as will be explained in the next section.



\subsection{Test of mass-profile recovery} \label{sec:tests.t2}

To get a means of comparing the simulated data (sims) to the generated modeled data (models), the total convergence, also called enclosed mass $\kappa_{\text{encl}}(r)$ for both was calculated.
The Einstein radius $\Theta_E$ is defined by $\kappa_{\text{encl}}(\Theta_E)=1$ and gives a number that allows the comparison between the sims and the models.
We also let an expert model three selected systems to compare the results from volunteers to those of a professional.

\todo{!} Should I write down prior expectation for T2 too?


To compare the enclosed mass profile and the Einstein radius of the simulation and the models, \kenc was calculated using the mass map \kap[x,y] directly generated in the modeling process.
From the ensemble of models generated by one modeling process, the mean is taken as the resulting $\kappa(r)$ to calculate $\theta_E$.
To estimate the errors, also the extremal models are used to estimate a lower and upper limit for $\theta_E$.
These results can be seen in \figref{ER_all_models}.
This figure shows that this technique of estimating the error minimizes the error significantly and should be improved for further analysis.


In \figref{ER_per_sim} can be seen, that the calculated \ERf of the models tend to be too high.
The overshoot varies from around 0.2 to 0.4 for good models.
One of the reasons for this is, that it's hard to get the center of the lens on spot.
An offset leads to a a flatter mass profile for the model compared to the simulation.





E05 and E06 happen mostly in very symmetric images, that are hard to come up with a unique valid solution.
This errors change the orientation of the mass distribution \kap[x,y],
but \kenc and thus the \ERf is not influenced that much, and thus the final result is still a valid model.
This can be seen by comparing the results for \asw{0h2m}, shown in \figref{kapenc_compare_faulty}: 
The correct model 7022 ($\ERm=10.76px$), which was done by an expert, and
7020 ($\ERm=10.72px$),
7024 ($\ERm=10.80px$),
7025 ($\ERm=11.16px$),
7021 ($\ERm=11.04px$).
The first three are of type E06, while the last is of type E05.
This can be easily corrected, if further analysis is done for the modeled system and time delays are known.


Comparing the models from volunteers and experts can be done in \figref{kapenc_compare_faulty}, where only the
expert got the right configuration, but all the resulting models are comparable, besides rotation.

Two additional sims were modeled by an expert, \asw{1hpf} and \asw{0vqg}.
Looking at the results for \ERg for those models in \figref{ER_per_sim}, we conclude that the performance of volunteers (blue crosses) and experts (red crosses, offset) is comparable.
Note that the models of \asw{0vqg} with \ERg[, rel] around 0.25 (6935 -- 6937) are failed models that show the attempts of a single user, that came finally up with model 6938 as final result.\todo{remove?}

\clearpage
